{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-DLgi7yGgqs"
   },
   "source": [
    "## This assignment is designed for automated pathology detection for Medical Images in a relalistic setup, i.e. each image may have multiple pathologies/disorders. \n",
    "### The goal, for you as an MLE, is to design models and methods to predictively detect pathological images and explain the pathology sites in the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KojRZzhGgqy"
   },
   "source": [
    "## Data for this assignment is taken from a Kaggle contest: https://www.kaggle.com/c/vietai-advance-course-retinal-disease-detection/overview\n",
    "Explanation of the data set:\n",
    "The training data set contains 3435 retinal images that represent multiple pathological disorders. The patholgy classes and corresponding labels are: included in 'train.csv' file and each image can have more than one class category (multiple pathologies).\n",
    "The labels for each image are\n",
    "\n",
    "```\n",
    "-opacity (0), \n",
    "-diabetic retinopathy (1), \n",
    "-glaucoma (2),\n",
    "-macular edema (3),\n",
    "-macular degeneration (4),\n",
    "-retinal vascular occlusion (5)\n",
    "-normal (6)\n",
    "```\n",
    "The test data set contains 350 unlabelled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nN9GQAtGgqz"
   },
   "source": [
    "# For this assignment, you are working with specialists for Diabetic Retinopathy and Glaucoma only, and your client is interested in a predictive learning model along with feature explanability and self-learning for Diabetic Retinopathy and Glaucoma vs. Normal images.\n",
    "# Design models and methods for the following tasks. Each task should be accompanied by code, plots/images (if applicable), tables (if applicable) and text:\n",
    "## Task 1: Build a classification model for Diabetic Retinopathy and Glaucoma vs normal images. You may consider multi-class classification vs. all-vs-one classification. Clearly state your choice and share details of your model, paremeters and hyper-paramaterization pprocess. (60 points)\n",
    "```\n",
    "a. Perform 70/30 data split and report performance scores on the test data set.\n",
    "b. You can choose to apply any data augmentation strategy. \n",
    "Explain your methods and rationale behind parameter selection.\n",
    "c. Show Training-validation curves to ensure overfitting and underfitting is avoided.\n",
    "```\n",
    "## Task 2: Visualize the heatmap/saliency/features using any method of your choice to demonstrate what regions of interest contribute to Diabetic Retinopathy and Glaucoma, respectively. (25 points)\n",
    "```\n",
    "Submit images/folder of images with heatmaps/features aligned on top of the images, or corresponding bounding boxes, and report what regions of interest in your opinion represent the pathological sites.\n",
    "```\n",
    "\n",
    "## Task 3: Using the unlabelled data set in the 'test' folder augment the training data (semi-supervised learning) and report the variation in classification performance on test data set.(15 points)\n",
    "[You may use any method of your choice, one possible way is mentioned below.] \n",
    "\n",
    "```\n",
    "Hint: \n",
    "a. Train a model using the 'train' split.\n",
    "b. Pass the unlabelled images through the trained model and retrieve the dense layer feature prior to classification layer. Using this dense layer as representative of the image, apply label propagation to retrieve labels correspndng to the unbalelled data.\n",
    "c. Next, concatenate the train data with the unlabelled data (that has now been self labelled) and retrain the network.\n",
    "d. Report classification performance on test data\n",
    "Use the unlabelled test data  to improve classification performance by using a semi-supervised label-propagation/self-labelling approach. (20 points)\n",
    "```\n",
    "## Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HieMaT_kHfbY"
   },
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import History, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, \\\n",
    "    Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vkYqE2U9AVaF"
   },
   "outputs": [],
   "source": [
    "# Global constants.\n",
    "DEFAULT_ROOT_DIR = os.path.join('.', 'Data')\n",
    "DEFAULT_DATASET_ARGS = {'val_split': 0.3}\n",
    "DEFAULT_MODEL_ARGS = {'input_shape': (512, 512, 3),\n",
    "                      'output_shape': (3,),\n",
    "                      'learning_rate': 1e-4}\n",
    "DEFAULT_TRAIN_ARGS = {'epochs': 5,\n",
    "                      'batch_size': 32,\n",
    "                      'shuffle': True,\n",
    "                      'use_tensorboard': False,\n",
    "                      'model_checkpoint_filename': None,\n",
    "                      'augment': False}\n",
    "METRICS = {'accuracy', 'precision', 'recall', 'F1'}\n",
    "DEFAULT_CLASS_NAMES = ['normal', 'diabetic retinopathy', 'glaucoma']\n",
    "TRAIN_KEY = 'train'\n",
    "VAL_KEY = 'val'\n",
    "TEST_KEY = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ua46Qe3Ggqz",
    "outputId": "b3c47c11-1133-40a0-fac1-6376b0616b9f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-338c5d32d9ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Your drive might be named 'gdrive' rather than 'drive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Midterm/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Your drive might be named 'gdrive' rather than 'drive'\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/Midterm/')\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTxriEbqAqga"
   },
   "source": [
    "# Task 1.\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will use a convolutional neural network with a 3-neuron softmax output. We make this choice of model because:\n",
    "\n",
    "* Convolutional neural networks are the state of the art in image classification problems, in almost every domain.\n",
    "* A 3-neuron sigmoid output (i.e., multi-label classification) is easy to implement for a neural network, and also allows us to quickly extend the model to 2, 7, or any arbitrary number of outputs.\n",
    "\n",
    "## Functions\n",
    "\n",
    "Below are the functions we will use to work with the dataset and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sS-NXwUeeqPg"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path: os.path) -> np.ndarray:\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "    :param path: The path to the image.\n",
    "    :return: The image contents as an np.ndarray of type uint8.\n",
    "    \"\"\"\n",
    "    img_data = cv2.imread(path)\n",
    "    img_data = cv2.cvtColor(img_data, cv2.COLOR_BGR2RGB)\n",
    "    (width, height) = img_data.shape[:2]\n",
    "    return img_data.reshape((height, width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XCgoZ2Ackiqz"
   },
   "outputs": [],
   "source": [
    "def normalize_image(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the normalized image from the numpy array.\n",
    "    :param img: The image to normalize, each element of which is np.uint8.\n",
    "    :return: The normalized image, each element of which is np.float16.\n",
    "    \"\"\"\n",
    "    if img.dtype != np.uint8:\n",
    "        raise ValueError('Expected image array type np.uint8, but found {0}'\n",
    "        .format(img.dtype))\n",
    "    return img.astype(np.float16) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OYP7ifc3lphi"
   },
   "outputs": [],
   "source": [
    "def denormalize_image(norm_img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the denormalized image (i.e., inverts the normalization process)\n",
    "    so that the output is a numpy array representing an image.\n",
    "    :param norm_img: The image to denormalize, each element of which is\n",
    "    np.float16.\n",
    "    :return: The denormalized image, each element of which is np.uint8.\n",
    "    \"\"\"\n",
    "    if norm_img.dtype != np.float16:\n",
    "        raise ValueError('Expected normalized image array type np.float16,'\n",
    "        'but found {0}'.format(img.dtype))\n",
    "    return (img * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dmbR5apqcyo"
   },
   "source": [
    "### A note on labels\n",
    "\n",
    "One decision I make on labels for this problem is that I do not force the labels for each example to contain at least one 1. In other words, it is possible for the label for an example to be (0, 0, 0), which means that the eye has no glaucoma, no retinopathy, and is not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "c0oXza6WjfCR"
   },
   "outputs": [],
   "source": [
    "def get_partition(root_dir: os.path, dataset_args: Dict[str, Any]) -> \\\n",
    "    Dict[str, List[str]]:\n",
    "    \"\"\"Returns a dict where the keys are 'train', 'test', and 'val', and the\n",
    "    values are the images under each. The list is the authoratative order of the\n",
    "    train/test examples; partition['train'][0] is the first training example,\n",
    "    and x_train[0] will correspond with that filename.\n",
    "    :param root_dir: The root directory of the dataset, in which are located\n",
    "    test and train subdirectories.\n",
    "    :param dataset_args: The dataset arguments. See DEFAULT_DATASET_ARGS for\n",
    "    available options.\n",
    "    :return: The train/val/test partition.\n",
    "    \"\"\"\n",
    "    dataset_args = {**DEFAULT_DATASET_ARGS, **dataset_args}\n",
    "    partition = {}\n",
    "    train_image_dir = os.path.join(root_dir, 'train', 'train')\n",
    "    train_image_filenames = [filename for\n",
    "                             filename in os.listdir(train_image_dir) if\n",
    "                             filename.endswith('.jpg')]\n",
    "    rand_indices = np.random.permutation(len(train_image_filenames))\n",
    "    split_index = int(dataset_args['val_split'] * len(train_image_filenames))\n",
    "    val_indices = rand_indices[:split_index]\n",
    "    train_indices = rand_indices[split_index:]\n",
    "    partition[TRAIN_KEY] = [train_image_filenames[i] for i in train_indices]\n",
    "    partition[VAL_KEY] = [train_image_filenames[i] for i in val_indices]\n",
    "    test_image_dir = os.path.join(root_dir, 'test', 'test')\n",
    "    test_image_filenames = [filename for\n",
    "                            filename in os.listdir(test_image_dir) if\n",
    "                            filename.endswith('.jpg')]\n",
    "    partition[TEST_KEY] = test_image_filenames\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rUxEd7_zLQKN"
   },
   "outputs": [],
   "source": [
    "def get_labels(root_dir: os.path, class_names: List[str]) -> Dict[str,\n",
    "                                                                  np.ndarray]:\n",
    "    \"\"\"Returns a dict where the keys are the image filenames and the values are\n",
    "    the labels. Each label has classes in the order of class_names (e.g.,\n",
    "    ['normal', 'diabetic retinopathy', 'glaucoma']).\n",
    "    :param root_dir: The root directory of the dataset, in which are located\n",
    "    test and train subdirectories.\n",
    "    :param class_names: The names of the classes, in order.\n",
    "    :return: The label dict.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    train_labels_filename = os.path.join(root_dir, 'train', 'train.csv')\n",
    "    df_train = pandas.read_csv(train_labels_filename)[['filename'] +\n",
    "                                                      class_names]\n",
    "    for i, filename in enumerate(df_train['filename']):\n",
    "        row = df_train.iloc[i]\n",
    "        labels[filename] = np.array([row[class_name] for \n",
    "                                     class_name in class_names],\n",
    "                                    dtype=np.float32)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "egccW0SSHvyW"
   },
   "outputs": [],
   "source": [
    "def _get_dataset_from_scratch(\n",
    "    root_dir: os.path,\n",
    "    partition: Dict[str, str],\n",
    "    labels: Dict[str, np.ndarray]) -> (np.ndarray, np.ndarray, np.ndarray,\n",
    "                                       np.ndarray):\n",
    "    \"\"\"Preprocesses and returns the dataset located at root_dir.\n",
    "    :param root_dir: The root directory of the dataset, in which are located\n",
    "    test and train subdirectories.\n",
    "    :param partition: The train/test partition.\n",
    "    :param labels: The labels.\n",
    "    :return: The preprocessed dataset as a 4-tuple: x_train, y_train, x_test,\n",
    "    y_test.\n",
    "    \"\"\"\n",
    "    train_image_dir = os.path.join(root_dir, 'train', 'train')\n",
    "    first_image = load_image_into_numpy_array(os.path.join(\n",
    "        train_image_dir, partition[TRAIN_KEY][0]))\n",
    "    input_shape = (len(partition[TRAIN_KEY]), *first_image.shape)\n",
    "    print('Inferring training input shape: {0}'.format(input_shape))\n",
    "    x_train = np.zeros(input_shape, dtype=np.float16)\n",
    "    y_train = np.zeros((input_shape[0], len(labels[partition[TRAIN_KEY][0]])))\n",
    "    for i, filename in enumerate(partition[TRAIN_KEY]):\n",
    "        if i % 100 == 0:\n",
    "            print('Loading image {0}'.format(i))\n",
    "        img_arr = load_image_into_numpy_array(os.path.join(\n",
    "            train_image_dir, filename))\n",
    "        preprocessed_arr = normalize_image(img_arr)\n",
    "        x_train[i, :] = preprocessed_arr\n",
    "        y_train[i, :] = labels[filename]\n",
    "    test_image_dir = os.path.join(root_dir, 'test', 'test')\n",
    "    first_image = load_image_into_numpy_array(os.path.join(\n",
    "        test_image_dir, partition[TEST_KEY][0]))\n",
    "    input_shape = (len(partition[TEST_KEY]), *first_image.shape)\n",
    "    if input_shape[1:] != x_train.shape[1:]:\n",
    "        raise ValueError('Expected test set shape, {0}, to match train set'\n",
    "        'shape, {1}.'.format(input_shape, x_train.shape))\n",
    "    print('Inferring test input shape: {0}'.format(input_shape))\n",
    "    x_test = np.zeros(input_shape, dtype=np.float16)\n",
    "    y_test = None\n",
    "    for i, filename in enumerate(partition[TEST_KEY]):\n",
    "        if i % 100 == 0:\n",
    "            print('Loading image {0}'.format(i))\n",
    "        img_arr = load_image_into_numpy_array(os.path.join(\n",
    "            test_image_dir, filename))\n",
    "        preprocessed_arr = normalize_image(img_arr)\n",
    "        x_test[i, :] = preprocessed_arr\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bmHDXwW5eAt7"
   },
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    root_dir: os.path,\n",
    "    partition: Dict[str, str],\n",
    "    labels: Dict[str, np.ndarray]) -> (np.ndarray, np.ndarray, np.ndarray,\n",
    "                                       np.ndarray):\n",
    "    \"\"\"Preprocesses and returns the dataset located at root_dir. If the cached\n",
    "    dataset files are available, uses those.\n",
    "    :param root_dir: The root directory of the dataset, in which are located\n",
    "    test and train subdirectories.\n",
    "    :param partition: The train/test partition.\n",
    "    :param labels: The labels.\n",
    "    :return: The preprocessed dataset as a 4-tuple: x_train, y_train, x_test,\n",
    "    y_test.\n",
    "    \"\"\"\n",
    "    return _get_dataset_from_scratch(root_dir, partition, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZzYq-GYQNLzK"
   },
   "outputs": [],
   "source": [
    "def _get_alex_net(model_args: Dict[str, Any]) -> Model:\n",
    "    \"\"\"Returns AlexNet as a tensorflow.keras.models.Model instance.\n",
    "    :param model_args: Model hyperparameters. See DEFAULT_MODEL_ARGS for\n",
    "    hyperparameters and their default values.\n",
    "    :return: A tensorflow.keras.models.Model instance.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, kernel_size=(11, 11), strides=4,\n",
    "                     padding='valid', activation='relu',\n",
    "                     input_shape=model_args['input_shape'],\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2),\n",
    "                           padding='valid', data_format=None))\n",
    "    model.add(Conv2D(256, kernel_size=(5, 5), strides=1,\n",
    "                     padding='same', activation='relu',\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2),\n",
    "                           padding='valid', data_format=None)) \n",
    "    model.add(Conv2D(384, kernel_size=(3, 3), strides=1,\n",
    "                     padding='same', activation= 'relu',\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(384, kernel_size=(3, 3), strides=1,\n",
    "                     padding='same', activation='relu',\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), strides=1,\n",
    "                     padding='same', activation='relu',\n",
    "                     kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2),\n",
    "                           padding='valid', data_format=None))\n",
    "    model.add(Flatten())\n",
    "    if len(model_args['output_shape']) != 1:\n",
    "        raise ValueError('Expected last layer to be flat, but found shape {0}'\n",
    "        .format(len(model_args['output_shape'])))\n",
    "    model.add(Dense(model_args['output_shape'][0], activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=model_args['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8cThhTYLFI3c"
   },
   "outputs": [],
   "source": [
    "def get_model(model_args: Dict[str, Any]) -> Model:\n",
    "    \"\"\"Returns a tensorflow.keras.models.Model instance for this dataset.\n",
    "    :param model_args: Model hyperparameters. See DEFAULT_MODEL_ARGS for\n",
    "    hyperparameters and their default values.\n",
    "    :return: A tensorflow.keras.models.Model instance.\n",
    "    \"\"\"\n",
    "    model_args = {**DEFAULT_MODEL_ARGS, **model_args}\n",
    "    return _get_alex_net(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "okZsxQwbGixi"
   },
   "outputs": [],
   "source": [
    "def train_model(model: Model,\n",
    "                partition: Dict[str, List[str]],\n",
    "                labels: Dict[str, np.ndarray],\n",
    "                train_args: Dict[str, Any],\n",
    "                root_dir: os.path,\n",
    "                class_names: List[str]) -> History:\n",
    "    \"\"\"Trains the model and returns the History object that results.\n",
    "    :param model: The model.\n",
    "    :param partition: The train/val/test partition.\n",
    "    :param labels: The labels.\n",
    "    :param train_args: Training hyperparameters. See DEFAULT_TRAIN_ARGS for\n",
    "    hyperparameters and their default values.\n",
    "    :param root_dir: The root directory of the dataset, in which are located\n",
    "    test and train subdirectories.\n",
    "    :param class_names: The names of the classes, in order.\n",
    "    :return: The History object that results from calling model.fit().\n",
    "    \"\"\"\n",
    "    train_args = {**DEFAULT_TRAIN_ARGS, **train_args}\n",
    "    df_train = pandas.DataFrame()\n",
    "    df_train['filename'] = partition[TRAIN_KEY]\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        df_train[class_name] = [labels[filename][i]\n",
    "                                for filename in partition[TRAIN_KEY]]\n",
    "    df_val = pandas.DataFrame()\n",
    "    df_val['filename'] = partition[VAL_KEY]\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        df_val[class_name] = [labels[filename][i]\n",
    "                              for filename in partition[VAL_KEY]]\n",
    "    if train_args['augment']:\n",
    "        image_datagen = ImageDataGenerator(\n",
    "            rotation_range=10.,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            zoom_range=0.2,\n",
    "            channel_shift_range=0.05,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant',\n",
    "            data_format='channels_last',\n",
    "            rescale=1 / 255)\n",
    "    else:\n",
    "        image_datagen = ImageDataGenerator(\n",
    "            data_format='channels_last',\n",
    "            rescale=1 / 255)\n",
    "    train_image_dir = os.path.join(root_dir, 'train', 'train')\n",
    "    train_generator = image_datagen.flow_from_dataframe(\n",
    "        df_train,\n",
    "        directory=train_image_dir,\n",
    "        x_col='filename',\n",
    "        y_col=class_names,\n",
    "        target_size=model.input_shape[1:3],\n",
    "        class_mode='raw',\n",
    "        batch_size=train_args['batch_size'],\n",
    "        shuffle=train_args['shuffle'])\n",
    "    val_generator = image_datagen.flow_from_dataframe(\n",
    "        df_val,\n",
    "        directory=train_image_dir,\n",
    "        x_col='filename',\n",
    "        y_col=class_names,\n",
    "        target_size=model.input_shape[1:3],\n",
    "        class_mode='raw',\n",
    "        batch_size=train_args['batch_size'],\n",
    "        shuffle=train_args['shuffle'])\n",
    "    callbacks = []\n",
    "    if train_args['use_tensorboard']:\n",
    "        log_dir = 'logs_{0}'.format(datetime.now())\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "        callbacks.append(tensorboard_callback)\n",
    "    if train_args['model_checkpoint_filename']:\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            train_args['model_checkpoint_filename'])\n",
    "        callbacks.append(checkpoint_callback)\n",
    "    return model.fit(\n",
    "        x=train_generator,\n",
    "        epochs=train_args['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_generator,\n",
    "        steps_per_epoch=math.ceil(len(partition[TRAIN_KEY]) /\n",
    "                                  train_args['batch_size']),\n",
    "        validation_steps=math.ceil(len(partition[VAL_KEY]) /\n",
    "                                   train_args['batch_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HIiCz0IgH-iT"
   },
   "outputs": [],
   "source": [
    "# TODO update to partition/labels.\n",
    "def eval_model(model: Model, x_test: np.ndarray, y_test: np.ndarray,\n",
    "               metric: str = 'accuracy') -> float:\n",
    "    \"\"\"Returns the model's performance on the test set as a single metric.\n",
    "    :param model: The model.\n",
    "    :param x_test: The preprocessed test dataset.\n",
    "    :param y_test: The test labels.\n",
    "    :param metric: The metric to use. One of METRICS (e.g., 'accuracy').\n",
    "    :return: The model's performance as a single metric.\n",
    "    \"\"\"\n",
    "    if metric not in METRICS:\n",
    "        raise ValueError('Expected metric to be one of {0}, but found {1}'\n",
    "        .format(METRICS, metric))\n",
    "    result = model.evaluate(\n",
    "        x=x_test,\n",
    "        y=y_test,\n",
    "        return_dict=True)\n",
    "    return result[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T78Wh4wNkvrK"
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points: List[float], factor: float = 0.6) -> List[float]:\n",
    "    \"\"\"Returns points smoothed over an exponential.\n",
    "    :param points: The points of the curve to smooth.\n",
    "    :param factor: The smoothing factor.\n",
    "    :return: The smoothed points.\n",
    "    \"\"\"\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            prev = smoothed_points[-1]\n",
    "            smoothed_points.append(prev * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3nvCLLqGLaX2"
   },
   "outputs": [],
   "source": [
    "def plot_history(history: History) -> None:\n",
    "    \"\"\"Plots the training history. You can also visualize the training process\n",
    "    in Tensorboard or wandb.\n",
    "    :param history: The training history.\n",
    "    \"\"\"\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, smooth_curve(acc, factor=smooth_fac), 'bo',\n",
    "             label='Smoothed training acc')\n",
    "    plt.plot(epochs, smooth_curve(val_acc, factor=smooth_fac), 'b',\n",
    "             label='Smoothed validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
    "    plt.plot(epochs, smooth_curve(val_loss), 'b',\n",
    "             label='Smoothed validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yvs2hfIzWfgy"
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Below we run experiments on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PyaFNrVMHB9",
    "outputId": "d818760a-a322-4a29-8f97-6a54ec7bdf66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2405 training images, 1030 val images, 350 test images.\n",
      "Found 3435 labels.\n"
     ]
    }
   ],
   "source": [
    "partition = get_partition(DEFAULT_ROOT_DIR, DEFAULT_DATASET_ARGS)\n",
    "labels = get_labels(DEFAULT_ROOT_DIR, DEFAULT_CLASS_NAMES)\n",
    "print('Found {0} training images, {1} val images, {2} test images.'.format(\n",
    "    len(partition[TRAIN_KEY]), len(partition[VAL_KEY]),\n",
    "    len(partition[TEST_KEY])))\n",
    "print('Found {0} labels.'.format(len(labels.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wM-Gq9bscLez"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters.\n",
    "model_args = {}\n",
    "train_args = {'batch_size': 4}\n",
    "metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxEZOR13hG9t",
    "outputId": "24af5493-fcd7-4ea7-a681-b1de118d183a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7f44ceeff0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m history = train_model(model, partition, labels, train_args, DEFAULT_ROOT_DIR,\n\u001b[1;32m      3\u001b[0m                       DEFAULT_CLASS_NAMES)\n\u001b[1;32m      4\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#eval_model(model, x_test, y_test, metric=metric)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model(model_args)\n",
    "history = train_model(model, partition, labels, train_args, DEFAULT_ROOT_DIR,\n",
    "                      DEFAULT_CLASS_NAMES)\n",
    "score = -1 #eval_model(model, x_test, y_test, metric=metric)\n",
    "plot_history(history)\n",
    "print('Model score ({0}): {1}'.format(metric, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynq1GA0DaPH5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML_for_Computer_Vision.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
